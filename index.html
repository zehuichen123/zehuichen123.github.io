<!DOCTYPE html>

<html><head>
<script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?168904e0fd0eba2a9ceee6886d50ec7c";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Zehui Chen's home page"> 
  
  <link href="./files/wfdoc.css" rel="stylesheet" type="text/css"> 
  <title>Zehui Chen's Homepage</title> 
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Zehui Chen &nbsp; 陈泽徽</h1></div>
        <h3>PhD Candidate</h3>
        <p>Brain-Inspired Vision Laboratory
        <br>School of Automation,
        <br>USTC
        <br>Hefei, China
        <br>
        <br> Email:  
        <a href="mailto:lovesnow@mail.ustc.edu.cn"> lovesnow@mail.ustc.edu.cn</a>; 
        <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:lovesnowbest@gmail.com">     lovesnowbest@gmail.com</a>; 
        <br> Google Scholar:  
        <a href="https://scholar.google.com/citations?user=NfSsLncAAAAJ&hl=en"> Google Scholar Link</a>
        <br> Github: 
        <a href="https://github.com/zehuichen123/">https://github.com/zehuichen123/</a> 
        <br><br></p>
      </td>
      <td>
        <div>
          <img width="170" src="./files/czh.png" border="0">
          <!-- <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> -->
        </div>
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">


  <h2>Biography</h2>
  <p> I am a 3rd-year PhD candidate at University of Science and Technology of China (USTC), advised by Prof. <a href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en">Feng Zhao</a>. I got a B.E. degree at Tongji University in 2020. Currently, I am leading the high-level vision group at <a href="https://bivlab123.github.io/">USTC-BIVLab</a>.
  </p>
  <p>My research interests include object detection (2D, 3D, or both of them), instance segmentation, and unsupervised learning.</p>
  <p><b>NOTE</b>: <i>Our Lab <a href="https://bivlab123.github.io/">[Link]</a> is looking forward to having elegant students or researchers join us. Positions for Master’s, Ph.D., and post-doc are opening. If you are interested in our research and want to join us, just contact me via email or WeChat(ID: lovesnowbest)!</i>

<h2>Experience</h2>
<ul>
  <li>
    Sep.2019 - Jul.2020, <i>Perception Research Intern</i>, <b>TuSimple</b>
  </li>

  <li>
    Oct.2020 - Mar.2021, <i>Computer Vision Intern</i>, <b>ByteDance</b>
  </li>

  <li>
    Mar.2021 - Now, <i>Perception Research Intern</i>, <b>SenseTime</b>
  </li>

</ul>

<h2>Awards</h2>
<ul>
   <li>
    National Scholarship. 2022.
  </li>
  <li>
    <b>3<sup>rd</sup></b> place at SSLAD 2022 Challenge, 3D Object Detection Track! (<b>ECCV 2022</b> Workshop)
  </li>
  <li>
    <b>2<sup>nd</sup></b> place at Mobile AI 2022 Challenge, Monocular Depth Estimation Track! (<b>ECCV 2022</b> Workshop)
  </li>
  <li>
    National Scholarship. 2021.
  </li>
  <li>
    <b>2<sup>nd</sup></b> place at Streaming Detection Challenge, Full Stack Track! (<b>CVPR 2021</b> Workshop)
  </li>
  <li>
    <b>3<sup>rd</sup></b> place at UG2+ Challenge, Low-Light Face Detection Track! (<b>CVPR 2021</b> Workshop)
  </li>
  <li>
    <b>1<sup>st</sup></b> place at 3D FUTURE Challenge, Instance Segmentation Track! (<b>IJCAI 2020</b> Workshop)
  </li>
  <li>
    <b>1<sup>st</sup></b> place at Waymo Open Challenge, 2D Detection Track! (<b>CVPR 2020</b> Workshop)
  </li>
</ul>



<h2>Publications</h2>
<h3>Preprint Papers</h3>
<table class="pub_table">
<tbody>
  <tr>
    <td class="pub_td1"><img src="./files/graphdetr4d.png" class="papericon"></td>
    <td class="pub_td2"><b>Graph-DETR4D: Spatial-Temporal Graph Modeling for Multi-View 3D Object Detection</b>
      <br><u>Zehui Chen</u>, Zheng Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao
      <!-- <br><i>Subitted to IEEE Transactions on Multimedia (<b>TMM</b>), 2023</i> -->
      <br><i>Under Review</i>
      <br>
      [<a href="">PDF</a>]
      [<a href="https://github.com/zehuichen123/Graph-DETR3D">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/bevinst.png" class="papericon"></td>
    <td class="pub_td2"><b> BEVInst: Improving Geometric Details in BEV Perception with Instance Representation</b>
      <br>Weijie Ma, Jingwei Jiang, Yang Yang, <u>Zehui Chen</u>, Hao Chen
      <br><i>Under Review</i><br>
      [<a href="">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/dgmono3d.png" class="papericon"></td>
    <td class="pub_td2"><br><b>Towards Model Generalization for Monocular 3D Object Detection</b>
      <br>Zhenyu Li, <u>Zehui Chen</u>, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang
      <br><i>Arxiv, 2022</i>
      <br>
      [<a href="https://arxiv.org/abs/2205.11664">PDF</a>]
    </td>
  </tr>

</tbody>
</table>
<table class="pub_table">
<tbody>
<h3>Published Papers</h3>
  <tr>
    <td class="pub_td1"><img src="./files/noisedet.png" class="papericon"></td>
    <td class="pub_td2"><b>Learning with Noisy Data for Semi-Supervised 3D Object Detection</b>
      <br><u>Zehui Chen</u>, Zhenyu Li, Shuo Wang, Dengpan Fu, Feng Zhao
      <br><i>International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
      [<a href="">PDF</a>]
      [<a href="https://github.com/zehuichen123/NoiseDet">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/ddod_tmm.png" class="papericon"></td>
    <td class="pub_td2"><b>DDOD: Dive Deeper into the Disentanglement of Object Detector</b>
      <br><u>Zehui Chen</u>, Chenhongyi Yang, Jiahao Chang, Feng Zhao, Zheng-Jun Zha, Feng Wu
      <br><i>IEEE Transactions on Multimedia (<b>TMM</b>)</i>
      <br>
      [<a href="">PDF</a>]
      [<a href="https://github.com/zehuichen123/DDOD">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/bevdistill.png" class="papericon"></td>
    <td class="pub_td2"><b>BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection</b>
      <br><u>Zehui Chen</u>, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao
      <br><i>International Conference on Learning Representations (<b>ICLR</b>), 2023</i><br>
      [<a href="https://arxiv.org/pdf/2211.09386.pdf">PDF</a>]
      [<a href="https://github.com/zehuichen123/BEVDistill">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/graphdetr3d.png" class="papericon"></td>
    <td class="pub_td2"><b>Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection</b>
      <br><u>Zehui Chen</u>, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao
      <br><i>ACM International Conference on Multimedia (<b>ACM MM</b>), 2022</i>
      <br>
      [<a href="https://arxiv.org/abs/2204.11582">PDF</a>]
      [<a href="https://github.com/zehuichen123/Graph-DETR3D">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/autoalignv2.png" class="papericon"></td>
    <td class="pub_td2"><b>AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection</b>
      <br><u>Zehui Chen</u>, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao
      <br><i>European Conference on Computer Vision (<b>ECCV</b>), 2022</i><br>
      [<a href="https://arxiv.org/abs/2207.10316">PDF</a>]
      [<a href="https://github.com/zehuichen123/AutoAlignV2">Code</a>]
    </td>
  </tr>

  <!-- #12 -->
  <tr>
    <td class="pub_td1"><img src="./files/autoalign.png" class="papericon"></td>
    <td class="pub_td2"><b>AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection</b>
      <br><u>Zehui Chen</u>, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao, Bolei Zhou, Hang Zhao
      <br><i>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2022</i>
      <br>
      [<a href="https://arxiv.org/abs/2201.06493">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/ddod.png" class="papericon"></td>
    <td class="pub_td2"><b>Disentangle Your Dense Object Detector</b>
      <br><u>Zehui Chen*</u>, Chenhongyi Yang*, Qiaofei Li, Feng Zhao, Zheng-Jun Zha, Feng Wu
      <br><i>ACM International Conference on Multimedia (<b>ACM MM</b>), 2021</i>
      <br>
      [<a href="https://arxiv.org/abs/2107.02963">PDF</a>]
      [<a href="https://github.com/zehuichen123/DDOD">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/detrdistill.png" class="papericon"></td>
    <td class="pub_td2"><b>DETRDistill: A Universal Knowledge Distillation Framework for DETR-families</b>
      <br>Jiahao Chang*, Shuo Wang*, Haiming Xu*, <u>Zehui Chen</u>, Chenhongyi Yang, Feng Zhao
      <br><i>International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
      [<a href="">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/dgnvdet.png" class="papericon"></td>
    <td class="pub_td2"><b>Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View</b>
      <br>Shuo Wang*, Xinhai Zhao*, Haiming Xu, <u>Zehui Chen</u>, Dameng Yu, Jiahao Chang, Zhen Yang, Feng Zhao
      <br><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023</i>
      <br>
      [<a href="">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/safe.png" class="papericon"></td>
    <td class="pub_td2"><b>SAFE: SIMULTANEOUS ALIGNMENT OF FEATURES AND PREDICTIONS FOR DENSE OBJECT DETECTORS</b>
      <br>Xuesong Guo*, Shuo Wang*, Jiahao Chang, <u>Zehui Chen</u>, Feng Zhao
      <br><i>IEEE International Conference on Multimedia and Expo (<b>ICME</b>), 2023</i>
      <br>
      [<a href="">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/candy.png" class="papericon"></td>
    <td class="pub_td2"><b>CANDY: CAtegory-kerNelized DYnamic Convolution for Instance Segmentation</b>
      <br>Yao Lu, Zhiyi Chen, <u>Zehui Chen</u>, Jie Hu, Liujuan Cao, ShengChuan Zhang
      <br><i>International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2023</i>
      <br>
      [<a href="">PDF</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/depthformer.png" class="papericon"></td>
    <td class="pub_td2"><br><b>DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation</b>
      <br>Zhenyu Li, <u>Zehui Chen</u>, Xianming Liu, Junjun Jiang
      <!-- <br><font color="red">Ranked <b>1<sup>st</sup></b> on KITTI depth estimation benchmark (Nov, 2021).</font> -->
      <br><i>Machine Intelligence Research, 2023</i>
      <br>
      [<a href="https://arxiv.org/abs/2203.14211">PDF</a>]
      [<a href="https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/litedepth.jpg" class="papericon"></td>
      <td class="pub_td2"><br><b>LiteDepth: Digging into Fast and Accurate Depth Estimation on Mobile Devices</b>
      <br>Zhenyu Li, <u>Zehui Chen</u>, Jialei Xu, Xianming Liu, Junjun Jiang
      <br><i>European Conference on Computer Vision Workshop (<b>ECCVW</b>), 2022</i>
      <br>
      [<a href="https://arxiv.org/abs/2209.00961">PDF</a>]
      [<a href="https://github.com/zhyever/LiteDepth">Code</a>]
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/stmono3d.png" class="papericon"></td>
    <td class="pub_td2"><b>Unsupervised Domain Adaptation for Monocular 3D Object Detection via Self-Training</b>
      <br>Zhenyu Li, <u>Zehui Chen</u>, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang
      <br><i>European Conference on Computer Vision (<b>ECCV</b>), 2022</i><br>
      <br>
      [<a href="https://arxiv.org/abs/2204.11590">PDF</a>]
      [<a href="https://github.com/zhyever/DAMono3D">Code</a>]
      <!-- [<a href="">Code</a>] -->
    </td>
  </tr>

  <tr>
    <td class="pub_td1"><img src="./files/simipu.png" class="papericon"></td>
    <td class="pub_td2"><b>SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for Spatial-Aware Visual Representations</b>
      <br>Zhenyu Li, <u>Zehui Chen</u>, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang, Bolei Zhou, Hang Zhao
      <br><i>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2022</i>
      <br>
      [<a href="https://arxiv.org/abs/2112.04680">PDF</a>]
      [<a href="https://github.com/zhyever/SimIPU">Code</a>]
      <!-- [<a href="">Code</a>] -->
    </td>
  </tr>

  </tbody>
</table>

    
<br>


</div>
</div>


</body></html>
